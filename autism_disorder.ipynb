{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning==1.7.5\n",
        "!pip install timm\n",
        "!pip install torchmetrics==0.11.4\n",
        "!pip install -U 'jsonargparse[signatures]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc6cn-DuRorF",
        "outputId": "b4581376-c918-4c4f-a0d1-0f25c3df72c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning==1.7.5\n",
            "  Downloading pytorch_lightning-1.7.5-py3-none-any.whl (706 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/706.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/706.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.6/706.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (2.15.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==1.7.5)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1 (from pytorch_lightning==1.7.5)\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.5) (4.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (3.9.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.5) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.5) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.5) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.5) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.5) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.5) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.*->pytorch_lightning==1.7.5)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.7.0->pytorch_lightning==1.7.5)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.5) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.*->pytorch_lightning==1.7.5) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->pytorch_lightning==1.7.5) (3.2.2)\n",
            "Installing collected packages: pyDeprecate, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# from lib.pos_embed import interpolate_pos_embed\n",
        "\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.cli import LightningCLI\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT, EPOCH_OUTPUT\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "# from lightning.pytorch.cli import ArgsType, LightningCLI\n",
        "from torchmetrics import Accuracy, ConfusionMatrix, AUROC\n",
        "from torch.optim import Optimizer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import timm\n",
        "from timm.data import Mixup\n",
        "from timm.models import create_model\n",
        "from timm.optim import create_optimizer_v2\n",
        "from timm.scheduler import create_scheduler\n",
        "from timm.scheduler.scheduler import Scheduler\n",
        "from timm.models.vision_transformer import VisionTransformer\n",
        "from timm.models.layers import PatchEmbed\n",
        "from timm.data import ImageDataset, create_transform\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "from pathlib import Path\n",
        "# from lib.augment import new_data_aug_generator\n",
        "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
        "from timm.data import ImageDataset, create_transform\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "\n",
        "from PIL import ImageFilter, ImageOps\n",
        "from torchvision import transforms\n"
      ],
      "metadata": {
        "id": "5GfZdWlsBmuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianBlur(object):\n",
        "    \"\"\"\n",
        "    Apply Gaussian Blur to the PIL image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.1, radius_min=0.1, radius_max=2.):\n",
        "        self.prob = p\n",
        "        self.radius_min = radius_min\n",
        "        self.radius_max = radius_max\n",
        "\n",
        "    def __call__(self, img):\n",
        "        do_it = random.random() <= self.prob\n",
        "        if not do_it:\n",
        "            return img\n",
        "\n",
        "        img = img.filter(\n",
        "            ImageFilter.GaussianBlur(\n",
        "                radius=random.uniform(self.radius_min, self.radius_max)\n",
        "            )\n",
        "        )\n",
        "        return img\n",
        "\n",
        "\n",
        "class Solarization(object):\n",
        "    \"\"\"\n",
        "    Apply Solarization to the PIL image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.2):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            return ImageOps.solarize(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "class GrayScale(object):\n",
        "    \"\"\"\n",
        "    Apply Grayscale transformation to the PIL image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.2):\n",
        "        self.p = p\n",
        "        self.transf = transforms.Grayscale(3)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            return self.transf(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "def new_data_aug_generator(args=None):\n",
        "    img_size = args.input_size\n",
        "    remove_random_resized_crop = args.src\n",
        "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "    primary_tfl = []\n",
        "    scale = (0.08, 1.0)\n",
        "    interpolation = 'bicubic'\n",
        "\n",
        "    if remove_random_resized_crop:\n",
        "        primary_tfl = [\n",
        "            transforms.Resize(img_size, interpolation=3),\n",
        "            transforms.RandomCrop(img_size, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "        ]\n",
        "    else:\n",
        "        primary_tfl = [\n",
        "            RandomResizedCropAndInterpolation(img_size, scale=scale, interpolation=interpolation),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "        ]\n",
        "\n",
        "    secondary_tfl = [transforms.RandomChoice([GrayScale(p=0.8),\n",
        "                                              Solarization(p=0.8),\n",
        "                                              GaussianBlur(p=0.8)])]\n",
        "\n",
        "    if args.color_jitter is not None and not args.color_jitter == 0:\n",
        "        secondary_tfl.append(transforms.ColorJitter(args.color_jitter, args.color_jitter, args.color_jitter))\n",
        "\n",
        "    final_tfl = [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=torch.tensor(mean),\n",
        "            std=torch.tensor(std))\n",
        "    ]\n",
        "\n",
        "    return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)"
      ],
      "metadata": {
        "id": "lBF7_mZ7LH8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed"
      ],
      "metadata": {
        "id": "KQONyGu4LH6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutismDatasetModule(LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 batch_size: int = 16,\n",
        "                 num_workers: int = 4,\n",
        "                 data_root: str = \"/kaggle/input/autism-gurv/data\",\n",
        "                 input_size: int = 224,\n",
        "                 color_jitter: float = 0.3,\n",
        "                 three_augment: bool = True,\n",
        "                 src: bool = False  # simple random crop\n",
        "                 ):\n",
        "        super(AutismDatasetModule, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.data_path = Path(self.hparams.data_root)\n",
        "        self.train_transforms = self.build_transform(is_train=True)\n",
        "        self.eval_transforms = self.build_transform(is_train=False)\n",
        "        if self.hparams.three_augment:\n",
        "            self.train_transforms = new_data_aug_generator(self.hparams)\n",
        "        self.class_map = {'autistic': 0, 'non_autistic': 1}\n",
        "\n",
        "    def build_transform(self, is_train):\n",
        "        resize_im = self.hparams.input_size > 32\n",
        "        t = []\n",
        "        if is_train:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(self.hparams.input_size, interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.RandomCrop(self.hparams.input_size, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ColorJitter(brightness=self.hparams.color_jitter,\n",
        "                                       contrast=self.hparams.color_jitter,\n",
        "                                       saturation=self.hparams.color_jitter),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
        "            ])\n",
        "            if not resize_im:\n",
        "                transform.transforms = transform.transforms[:-5]  # Remove Resize for small input sizes\n",
        "            return transform\n",
        "        else:\n",
        "            t.append(transforms.Resize(int(1.0 * self.hparams.input_size), interpolation=InterpolationMode.BICUBIC))\n",
        "            t.append(transforms.CenterCrop(self.hparams.input_size))\n",
        "        t.append(transforms.ToTensor())\n",
        "        t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "        return transforms.Compose(t)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = ImageDataset(str(self.data_path / \"train\"), transform=self.train_transforms,\n",
        "                                     class_map=self.class_map)\n",
        "        train_sampler = RandomSampler(train_dataset)\n",
        "        return DataLoader(train_dataset,\n",
        "                          sampler=train_sampler,\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True,\n",
        "                          drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = ImageDataset(str(self.data_path / \"valid\"), transform=self.eval_transforms,\n",
        "                                   class_map=self.class_map)\n",
        "        val_sampler = SequentialSampler(val_dataset)\n",
        "        return DataLoader(val_dataset,\n",
        "                          sampler=val_sampler,\n",
        "                          batch_size=4,\n",
        "                          num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True,\n",
        "                          drop_last=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataset = ImageDataset(str(self.data_path / \"test\"), transform=self.eval_transforms,\n",
        "                                    class_map=self.class_map)\n",
        "        test_sampler = SequentialSampler(test_dataset)\n",
        "        return DataLoader(test_dataset,\n",
        "                          sampler=test_sampler,\n",
        "                          batch_size=4,\n",
        "                          num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True,\n",
        "                          drop_last=False)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        predict_dataset = ImageDataset(str(self.data_path), transform=self.eval_transforms,\n",
        "                                       class_map=self.class_map)\n",
        "        predict_sampler = SequentialSampler(predict_dataset)\n",
        "        return DataLoader(predict_dataset,\n",
        "                          sampler=predict_sampler,\n",
        "                          batch_size=1,\n",
        "                          num_workers=self.hparams.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True,\n",
        "                          drop_last=False)"
      ],
      "metadata": {
        "id": "3pVmspbPMZWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTASD(nn.Module):\n",
        "    def __init__(self, backbone: str, num_classes, drop_rate, drop_path_rate, input_size):\n",
        "        super(ViTASD, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.backbone = create_model(\n",
        "            backbone,\n",
        "            pretrained=True,\n",
        "            num_classes=num_classes,\n",
        "            drop_rate=drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            drop_block_rate=None,\n",
        "            img_size=input_size\n",
        "        )\n",
        "        self.embed_dim = self.backbone.embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.patch_embed(x)\n",
        "        x = x + self.backbone.pos_embed\n",
        "        x = torch.cat([self.backbone.cls_token.expand(x.shape[0], -1, -1), x], dim=1)\n",
        "        x = self.backbone.blocks(x)\n",
        "        x = self.backbone.norm(x)\n",
        "        x = x[:, 0]  # Extract the representation of the CLS token\n",
        "        x = self.backbone.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z4a7fQd7Nqyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTASDLM(LightningModule):\n",
        "    def __init__(self,\n",
        "                 batch_size: int = 256,\n",
        "                 num_classes: int = 2,\n",
        "                 epochs: int = 300,\n",
        "                 attn_only: bool = False,\n",
        "                 smoothing: float = 0.0,  # Label smoothing\n",
        "                 vis_path: str = \"./runs/vis\",\n",
        "\n",
        "                 # Model parameters\n",
        "                 model: str = \"deit3_base_patch16_224\",  # Name of model to train\n",
        "                 input_size: int = 224,  # images input size\n",
        "                 drop: float = 0.0,  # Dropout rate\n",
        "                 drop_path: float = 0.05,  # Drop path rate\n",
        "                 pretrain_path: str = \"\"\n",
        "\n",
        "                 # Optimizer parameters\n",
        "                 opt: str = \"adamw\",\n",
        "                 weight_decay: float = 0.05,\n",
        "\n",
        "                 # Learning rate schedule parameters\n",
        "                 sched: str = \"cosine\",\n",
        "                 lr: float = 1e-4,\n",
        "                 warmup_lr: float = 1e-6,\n",
        "                 min_lr: float = 1e-6,\n",
        "                 warmup_epochs: int = 5,  # epochs to warmup LR, if scheduler supports\n",
        "                 cooldown_epochs: int = 0,  # epochs to cooldown LR at min_lr, after cyclic schedule ends\n",
        "\n",
        "                 # Mixup parameters\n",
        "                 mixup: float = 0.8,  # mixup alpha, mixup enabled if > 0\n",
        "                 cutmix: float = 1.0,  # cutmix alpha, cutmix enabled if > 0.\n",
        "                 mixup_prob: float = 1.0,  # Prob of performing mixup or cutmix when either/both is enabled\n",
        "                 mixup_switch_prob: float = 0.5,  # Prob of switching to cutmix when both mixup and cutmix enabled\n",
        "                 mixup_mode: str = \"batch\",  # How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"\n",
        "                 ):\n",
        "\n",
        "        super(ViTASDLM, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.model: torch.nn.Module = ViTASD(\n",
        "            self.hparams.model,\n",
        "            num_classes=self.hparams.num_classes,\n",
        "            drop_rate=self.hparams.drop,\n",
        "            drop_path_rate=self.hparams.drop_path,\n",
        "            input_size=self.hparams.input_size\n",
        "        )\n",
        "\n",
        "        if os.path.exists(pretrain_path):\n",
        "            self._load_pretrained(pretrain_path)\n",
        "\n",
        "        self._init_mixup()\n",
        "        self._init_frozen_params()\n",
        "        self.train_criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.valid_criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.valid_acc = Accuracy()\n",
        "        self.auroc = AUROC(num_classes=2)\n",
        "        self.confusion_matrix = ConfusionMatrix(num_classes=self.hparams.num_classes, normalize='true')\n",
        "\n",
        "    def _init_mixup(self):\n",
        "        self.mixup_fn = None\n",
        "        mixup_active = self.hparams.mixup > 0 or self.hparams.cutmix > 0.\n",
        "        if mixup_active:\n",
        "            self.mixup_fn = Mixup(\n",
        "                mixup_alpha=self.hparams.mixup,\n",
        "                cutmix_alpha=self.hparams.cutmix,\n",
        "                cutmix_minmax=None,\n",
        "                prob=self.hparams.mixup_prob,\n",
        "                switch_prob=self.hparams.mixup_switch_prob,\n",
        "                mode=self.hparams.mixup_mode,\n",
        "                label_smoothing=self.hparams.smoothing,\n",
        "                num_classes=self.hparams.num_classes\n",
        "            )\n",
        "\n",
        "    def _load_pretrained(self, pretrain_path):\n",
        "        checkpoint = torch.load(pretrain_path)\n",
        "        print(\"Load pre-trained checkpoint from: %s\" % pretrain_path)\n",
        "        checkpoint_model = checkpoint['state_dict']\n",
        "        state_dict = self.model.state_dict()\n",
        "        for k in ['backbone.head.weight', 'backbone.head.bias']:\n",
        "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "                del checkpoint_model[k]\n",
        "        # interpolate position embedding\n",
        "        interpolate_pos_embed(self.model, checkpoint_model)\n",
        "        self.model.load_state_dict(checkpoint_model, strict=False)\n",
        "\n",
        "\n",
        "    def _init_frozen_params(self):\n",
        "        if self.hparams.attn_only:\n",
        "            for name_p, p in self.model.named_parameters():\n",
        "                if '.attn.' in name_p:\n",
        "                    p.requires_grad = True\n",
        "                else:\n",
        "                    p.requires_grad = False\n",
        "\n",
        "            self.model.backbone.head.weight.requires_grad = True\n",
        "            self.model.backbone.head.bias.requires_grad = True\n",
        "            self.model.backbone.pos_embed.requires_grad = True\n",
        "            for p in self.model.backbone.patch_embed.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
        "        samples, targets = batch\n",
        "        if self.mixup_fn is not None:\n",
        "            samples, targets = self.mixup_fn(samples, targets)\n",
        "        outputs = self.forward(samples)\n",
        "        loss = self.train_criterion(outputs, targets)\n",
        "        loss_value = loss.item()\n",
        "        self.log('Loss/train', loss_value, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
        "        samples, targets = batch\n",
        "        outputs = self.forward(samples)\n",
        "        loss = self.valid_criterion(outputs, targets)\n",
        "        loss_value = loss.item()\n",
        "        self.valid_acc.update(outputs, targets)\n",
        "        self.log(\"Accuracy/val\", self.valid_acc, on_step=True, on_epoch=True, sync_dist=True)\n",
        "        # self.log(\"AUROC/val\", self.auroc(outputs, targets), on_epoch=True, sync_dist=True)\n",
        "        self.log(\"Loss/val\", loss_value, sync_dist=True)\n",
        "        return self.valid_acc\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n",
        "        samples, targets = batch\n",
        "        outputs = self.forward(samples)\n",
        "        self.confusion_matrix.update(outputs, targets)\n",
        "\n",
        "    def training_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:\n",
        "        opt: Optimizer = self.optimizers()\n",
        "        self.log(\"LR\", opt.param_groups[0][\"lr\"], on_epoch=True, sync_dist=True)\n",
        "\n",
        "    def on_test_end(self) -> None:\n",
        "        self.visualize_confusion_matrix()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = create_optimizer_v2(\n",
        "            self.model,\n",
        "            opt=self.hparams.opt,\n",
        "            lr=self.hparams.lr,\n",
        "            weight_decay=self.hparams.weight_decay,\n",
        "        )\n",
        "        scheduler, _ = create_scheduler(self.hparams, optimizer)\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
        "\n",
        "    def lr_scheduler_step(self, scheduler: Scheduler, optimizer_idx, metric) -> None:\n",
        "        scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value\n",
        "\n",
        "    def visualize_confusion_matrix(self):\n",
        "        cf_matrix = self.confusion_matrix.compute().cpu()\n",
        "        categories = [f'C{i}' for i in range(self.hparams.num_classes)]\n",
        "        fig, ax = plt.subplots(1)\n",
        "        sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='.2f', xticklabels=categories, yticklabels=categories)\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True Label')\n",
        "        vis_path = Path(self.hparams.vis_path)\n",
        "        fig.savefig(str(vis_path / f\"cf_matrix.png\"), dpi=200)\n",
        ""
      ],
      "metadata": {
        "id": "HuQGR99mLH3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks configuration\n",
        "callbacks = [\n",
        "    {\n",
        "        \"class_path\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
        "        \"init_args\": {\n",
        "            \"dirpath\": \"/kaggle/working/\",\n",
        "            \"save_top_k\": 3,\n",
        "            \"monitor\": \"Accuracy/val\",\n",
        "            \"mode\": \"max\",\n",
        "            \"save_weights_only\": True,\n",
        "            \"auto_insert_metric_name\": True\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Instantiating the ModelCheckpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(**callbacks[0][\"init_args\"])\n",
        "\n",
        "# Logger configuration\n",
        "logger_config = {\n",
        "    \"class_path\": \"pytorch_lightning.loggers.TensorBoardLogger\",\n",
        "    \"init_args\": {\n",
        "        \"save_dir\": \"/kaggle/working/lightning_logs\",\n",
        "        \"name\": \"ViTASD-L\",\n",
        "        \"version\": 3,\n",
        "        \"default_hp_metric\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "# Instantiating the TensorBoardLogger\n",
        "logger_callback = TensorBoardLogger(**logger_config[\"init_args\"])\n"
      ],
      "metadata": {
        "id": "0o1NCX21LH0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    accelerator=\"ddp\",  # Use Distributed Data Parallel (DDP) for multi-GPU training\n",
        "    gpus=[0, 1],  # Utilize GPUs with IDs 0 and 1\n",
        "    min_epochs=1,\n",
        "    max_epochs=300,\n",
        "    precision=32,\n",
        "    num_nodes=1,\n",
        "    check_val_every_n_epoch=1,\n",
        "    overfit_batches=0.0,\n",
        "    default_root_dir=os.path.join(\"/kaggle\", \"input\", \"autism-gurv\", \"data\"),\n",
        "    enable_checkpointing=True,\n",
        "    log_every_n_steps=30,\n",
        "    enable_progress_bar=True,\n",
        "    sync_batchnorm=False,\n",
        "    track_grad_norm=-1,\n",
        "    enable_model_summary=True,\n",
        "    num_sanity_val_steps=2,\n",
        "    reload_dataloaders_every_n_epochs=0,\n",
        "    replace_sampler_ddp=True,\n",
        "    auto_lr_find=False,\n",
        "    detect_anomaly=False,\n",
        "    auto_scale_batch_size=False,\n",
        "    move_metrics_to_cpu=False,\n",
        "    amp_backend=\"native\",\n",
        "    multiple_trainloader_mode=\"max_size_cycle\",\n",
        "    callbacks=[checkpoint_callback],  # Use the previously defined ModelCheckpoint callback\n",
        "    logger=logger_callback  # Use the previously defined TensorBoardLogger\n",
        ")"
      ],
      "metadata": {
        "id": "tVePYd-9LHyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTASDLM()"
      ],
      "metadata": {
        "id": "fRuup78WLHvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm = AutismDatasetModule()"
      ],
      "metadata": {
        "id": "p0nMkwtWLHtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, dm)"
      ],
      "metadata": {
        "id": "lbH_dQwuLHqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Gar17gBLHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9VHMvjELHln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}